# -*- coding: utf-8 -*-
"""DistillationTeacher.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nudc4Oa0iAbLJMqPxYPH9KL2jV4tCSc7
"""

import os
from __future__ import absolute_import, division, print_function, unicode_literals
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow.compat.v2 as tf2
import numpy as np
import tensorflow.contrib.slim as slim
from tensorflow.examples.tutorials.mnist import input_data

tf2.enable_v2_behavior()
print(tf2.__version__)


def MnistTeacher(input,keep_prob_conv, keep_prob_hidden, scope = 'Mnist',reuse = False):
  with tf2.variable_creator_scope(scope, reuse = reuse) as sc:
    with slim.arg_scope([slim.conv2d],kernel_size = [3,3],stride = [1,1], biases_initializer = tf2.constant_initializer(0.0),activation_fn= tf2.nn.relu):

      net = slim.conv2d(input ,32,scope= 'conv1')
      net = slim.max_pool2d(net,[2,2],2, scope= 'pool1')
      net = tf2.nn.dropout(net,kepp_prob_conv)


      net = slim.conv2d(net,64, scope='conv2')
      net = slim.max_pool2d(net,[2,2,],2, scope='pool2')
      net = tf2.nn.dropout(net,keep_prob_conv)

      net = slim.conv2d(net, 128, scope='conv3' )
      net = slim.max_pool2d(net,[2,2],2, scope='pool3')
      net = tf2.nn.dropout(net, keep_prob_conv)

      net = slim.flatten(net)
    with slim.arg_scope([slim.fully_connected],biases_initializer = tf2.constant_initializer(0,0),activation_fn = tf2.nn.relu):
     net = slim.fully_connected(net, 625, scope = 'fc1')
     net = tf2.nn.dropout(net, keep_prob_hidden)
     net = slim.fully_connected(net, 10, activation_fn= None, scope= 'fc2')

     net = tf2.nn.softmax(net,temperature)
     return net
     

def MnistStudent (input, scope = "Mnist", reuse = False):
  with tf2.variable_creator_scope(scope, reuse = reuse) as sc:
    with slim.arg_scope([slim.fully_connected], biases_initializer = tf2.constant_initializer(0,0), activation_fn = tf2.nn.sigmoid):


      net = slim.fully_connected(input, 1000,scope='fc1')
      net = slim.fully_connected(net, 10, activation_fn= None, scope= 'fc2')
      return net

def loss(prediction, output, temperature = 1):
  cross_entropy = tf2.reduce_mean(-tf2.reduce_sum(output * tf2.log(prediction), reduction_indices = [1]))
  correcr_prediction = tf2.equal(tf2.argmax(prediction,1), tf2.argmax(ouuput,1))
  accuracy = tf2.reduce_mean(tf2.cast(correcr_prediction,tf2.float32))
  return cross_entropy, accuracy

  eps =0.1
  alpha = 0.5
  temperature = 1
  start_lr = 1e-4
  decay =1e-6

with tf2.Graph().as_default():

  mnist = input_data.read_data_sets("MNIST_data/", one_hot =  True)

  x = tf2.placeholder(tf2.float32, shape = [None, 784])
  y_ = tf2.placeholder(tf2.float32, shape [None, 10])
  keep_prob_conv = tf2.placeholder(tf2.float32)
  keep_prob_hidden = tf2.placeholder(tf2.float32)
  x_image = tf2.reshape(x, [-1,28,28,1])

  y_conv_teacher = MnistTeacher(x_image, keep_prob_conv, keep_prob_hidden, scope = "teacher")
  y_conv = MnistStudent(x, scope = 'student')

  y_conv_student = tf2.nn.softmax(y_conv/temperature)
  y_conv_student_actual = tf2.nn.softmax(y_conv)

  cross_entropy_teacher, accuracy_teacher = loss(y_conv_teacher,y_, temperature = temperature)
  student_loss1, accuracy_student = loss(y_conv_student_actual,y_, temperature = temperature)

  student_loss2 = tf2.reduce_mean(- tf2.reduce_sum(y_conv_teacher * tf2.log(y_conv_student), reduction_indices=1))
  cross_entropy_student=student_student_loss2
  
 	model_vars = tf2.trainable_variables()
	var_teacher = [var for var in model_vars if 'teacher' in var.name]
	var_student = [var for var in model_vars if 'student' in var.name]

	grad_teacher = tf2.gradients(cross_entropy_teacher,var_teacher)
	grad_student = tf2.gradients(cross_entropy_student,var_student)
 
	l_rate = tf2.placeholder(shape=[],dtype = tf2.float32)
	
	trainer = tf2.train.RMSPropOptimizer(learning_rate = l_rate)
	trainer1 = tf2.train.GradientDescentOptimizer(0.1)

	train_step_teacher = trainer.apply_gradients(zip(grad_teacher,var_teacher))
  train_step_student = trainer1.apply_gradients(zip(grad_student,var_student))

  sess = tf2.InteractiveSession()
 	sess.run(tf2.global_variables_initializer())
 	saver1 = tf2.train.Saver(var_teacher)
 	saver2 = tf2.train.Saver(var_student)
	
  for i in range(20000):
     batch = mnist.train.next_batch(128)
	   lr = start_lr * 1.0/(1.0 + i*decay)
	   if i%50 == 0:
		   train_accuracy = accuracy_teacher.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob_conv: 1.0,keep_prob_hidden: 1.0})
		   print("step %d, training accuracy %g"%(i, train_accuracy))
	     train_step_teacher.run(feed_dict={x: batch[0], y_: batch[1], keep_prob_conv :0.8,keep_prob_hidden:0.5,l_rate:lr})
	
	saver1.save(sess,'./models/teacher1.ckpt')
	
	 
	for i in range(15000):
   batch = mnist.train.next_batch(100)
	 if i%50 == 0:
     train_accuracy = accuracy_student.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob_conv: 1.0,keep_prob_hidden: 1.0})
		 print("step %d, training accuracy %g"%(i, train_accuracy))
	 train_step_student.run(feed_dict={x: batch[0], y_: batch[1], keep_prob_conv :1.0,keep_prob_hidden:1.0})

	  
	saver2.save(sess,'./models/student.ckpt')  
	
	test_acc = sess.run(accuracy_student,feed_dict={x:mnist.test.images, y_: mnist.test.labels, keep_prob_conv: 1.0,keep_prob_hidden: 1.0})
	print("test accuracy of the student model is %g "%(test_acc))